{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=cpu,floatX=float64\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.special as sc\n",
    "\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "\n",
    "from pymc3 import Dirichlet, Poisson, Gamma\n",
    "from pymc3 import math as pmmath\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from theano import shared\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "\n",
    "# unfortunately I was not able to run it on GPU due to overflow problems\n",
    "%env THEANO_FLAGS=device=cpu,floatX=float64\n",
    "\n",
    "plt.style.use(\"seaborn-darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAEncoder:\n",
    "    \"\"\"Encode (term-frequency) document vectors to variational means and (log-transformed) stds.\"\"\"\n",
    "\n",
    "    def __init__(self, n_words, n_hidden, n_topics, p_corruption=0, random_seed=1):\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        self.n_words = n_words\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_topics = n_topics\n",
    "        self.w0 = shared(0.01 * rng.randn(n_words, n_hidden).ravel(), name=\"w0\")\n",
    "        self.b0 = shared(0.01 * rng.randn(n_hidden), name=\"b0\")\n",
    "        self.w1 = shared(0.01 * rng.randn(n_hidden, 2 * (n_topics)).ravel(), name=\"w1\")\n",
    "        self.b1 = shared(0.01 * rng.randn(2 * (n_topics)), name=\"b1\")\n",
    "        self.rng = MRG_RandomStreams(seed=random_seed)\n",
    "        self.p_corruption = p_corruption\n",
    "\n",
    "    def encode(self, xs):\n",
    "        if 0 < self.p_corruption:\n",
    "            dixs, vixs = xs.nonzero()\n",
    "            mask = tt.set_subtensor(\n",
    "                tt.zeros_like(xs)[dixs, vixs],\n",
    "                self.rng.binomial(size=dixs.shape, n=1, p=1 - self.p_corruption),\n",
    "            )\n",
    "            xs_ = xs * mask\n",
    "        else:\n",
    "            xs_ = xs\n",
    "\n",
    "        w0 = self.w0.reshape((self.n_words, self.n_hidden))\n",
    "        w1 = self.w1.reshape((self.n_hidden, 2 * self.n_topics))\n",
    "        hs = tt.tanh(xs_.dot(w0) + self.b0)\n",
    "        zs = hs.dot(w1) + self.b1\n",
    "        zs_mean = zs[:, :self.n_topics]\n",
    "        zs_rho = zs[:, self.n_topics: ]\n",
    "        return {\"mu\": zs_mean, \"rho\": zs_rho}\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.w0, self.b0, self.w1, self.b1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.258s.\n",
      "Extracting tf features for LDA...\n",
      "done in 1.407s.\n"
     ]
    }
   ],
   "source": [
    "n_words = 1000\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "data_samples = dataset.data\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_words, stop_words=\"english\")\n",
    "\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs for training = 10000\n",
      "Number of docs for test = 1314\n",
      "Number of tokens in training set = 480280\n",
      "Sparsity = 0.0253937\n"
     ]
    }
   ],
   "source": [
    "n_samples_tr = 10000\n",
    "n_samples_te = tf.shape[0] - n_samples_tr\n",
    "docs_tr = tf[:n_samples_tr, :]\n",
    "docs_te = tf[n_samples_tr:, :]\n",
    "print(\"Number of docs for training = {}\".format(docs_tr.shape[0]))\n",
    "print(\"Number of docs for test = {}\".format(docs_te.shape[0]))\n",
    "\n",
    "n_tokens = np.sum(docs_tr[docs_tr.nonzero()])\n",
    "print(f\"Number of tokens in training set = {n_tokens}\")\n",
    "print(\n",
    "    \"Sparsity = {}\".format(len(docs_tr.nonzero()[0]) / float(docs_tr.shape[0] * docs_tr.shape[1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp_lda_doc(beta, theta, n):\n",
    "    \"\"\"Returns the log-likelihood function for given documents.\n",
    "\n",
    "    K : number of topics in the model\n",
    "    V : number of words (size of vocabulary)\n",
    "    D : number of documents (in a mini-batch)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : tensor (K x V)\n",
    "        Word distributions.\n",
    "    theta : tensor (D x K)\n",
    "        Topic distributions for documents.\n",
    "    n: tensor (D x 1)\n",
    "        Lengths of each documents\n",
    "    \"\"\"\n",
    "\n",
    "    def ll_docs_f(docs):\n",
    "        \n",
    "        \"\"\"\n",
    "        \\log p(d | theta, beta, n) = \\sum_w \\log p(w | theta, beta)\n",
    "                                   = \\sum_w \\log Poisson(w | theta @ beta * n) \n",
    "                                   = \\sum_w \\log Poisson(w | \\sum_k theta_k * beta_k)\n",
    "                                   = \\sum_w \\log Poisson(w | \\sum_k \\exp( \\log theta_k + \\log beta_k ))\n",
    "                                   = \\sum_w - \\sum_k \\exp( \\log theta_k + \\log beta_k ) + w * \\log \\sum_k \\exp( \\log theta_k + \\log beta_k ) - \\log gamma(w + 1)\n",
    "        \"\"\"\n",
    "        dixs, vixs = docs.nonzero()\n",
    "        n = docs.sum(1)\n",
    "        vfreqs = docs[dixs, vixs]\n",
    "        ll_docs = (\n",
    "            (theta[dixs] + beta.T[vixs]).sum(1) *  n[dixs] + \n",
    "            vfreqs * n[dixs] * pmmath.logsumexp(tt.log(theta[dixs]) + tt.log(beta.T[vixs]), axis=1).ravel() - \\\n",
    "            pm.distributions.special.gammaln(vfreqs + 1)\n",
    "        )\n",
    "        # Per-word log-likelihood times num of tokens in the whole dataset\n",
    "        return tt.sum(ll_docs) / (tt.sum(vfreqs) + 1e-9) * n_tokens\n",
    "\n",
    "    return ll_docs_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Poisson.dist(\n",
    "    mu=pm.floatX(avg_len) * np.ones((minibatch_size, 1)),\n",
    "    shape=(minibatch_size, 1),\n",
    ").random()\n",
    "\n",
    "beta = Dirichlet.dist(\n",
    "    a=pm.floatX((1.0 / n_topics) * np.ones((n_topics, n_words))),\n",
    "    shape=(n_topics, n_words),\n",
    ").random()\n",
    "\n",
    "theta = Dirichlet.dist(\n",
    "    a=pm.floatX((1.0 / n_topics) * np.ones((minibatch_size, n_topics))),\n",
    "    shape=(minibatch_size, n_topics),\n",
    "    # do not forget scaling\n",
    ").random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutianc/miniconda3/lib/python3.7/site-packages/pymc3/data.py:246: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  self.shared = theano.shared(data[in_memory_slc])\n",
      "/home/liutianc/miniconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n"
     ]
    }
   ],
   "source": [
    "n_topics = 10\n",
    "minibatch_size = 32\n",
    "avg_len = docs_tr.sum(1).mean()\n",
    "\n",
    "doc_t_minibatch = pm.Minibatch(docs_tr.toarray(), minibatch_size)\n",
    "doc_t = shared(docs_tr.toarray()[:minibatch_size])\n",
    "\n",
    "\n",
    "e0 = c0 = 1.\n",
    "f0 = .01\n",
    "pn = .5\n",
    "\n",
    "with pm.Model() as model:\n",
    "    n = Poisson(\n",
    "        \"n\", \n",
    "        mu=pm.floatX(avg_len) * np.ones((minibatch_size, 1)),\n",
    "        shape=(minibatch_size, 1),\n",
    "        total_size=n_samples_tr\n",
    "    )\n",
    "    \n",
    "    beta = Dirichlet(\n",
    "        \"beta\",\n",
    "        a=pm.floatX((1.0 / n_topics) * np.ones((n_topics, n_words))),\n",
    "        shape=(n_topics, n_words),\n",
    "    )\n",
    "\n",
    "    theta = Dirichlet(\n",
    "        \"theta\",\n",
    "        a=pm.floatX((1.0 / n_topics) * np.ones((minibatch_size, n_topics))),\n",
    "        shape=(minibatch_size, n_topics),\n",
    "        # do not forget scaling\n",
    "        total_size=n_samples_tr,\n",
    "    )\n",
    "    \n",
    "    # Note, that we defined likelihood with scaling, so here we need no additional `total_size` kwarg\n",
    "    doc = pm.DensityDist(\"doc\", logp_lda_doc(beta, theta, n), observed=doc_t)\n",
    "\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(1000, step)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
