{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=cpu,floatX=float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutianc/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/liutianc/miniconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.special as sc\n",
    "\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "\n",
    "from pymc3 import Dirichlet, Poisson, Gamma, Bernoulli, Normal\n",
    "from pymc3 import math as pmmath\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from theano import shared\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "\n",
    "# unfortunately I was not able to run it on GPU due to overflow problems\n",
    "%env THEANO_FLAGS=device=cpu,floatX=float64\n",
    "\n",
    "plt.style.use(\"seaborn-darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAEncoder:\n",
    "    \"\"\"Encode (term-frequency) document vectors to variational means and (log-transformed) stds.\"\"\"\n",
    "\n",
    "    def __init__(self, n_words, n_hidden, n_topics, p_corruption=0, random_seed=1):\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        self.n_words = n_words\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_topics = n_topics\n",
    "        self.w0 = shared(0.01 * rng.randn(n_words, n_hidden).ravel(), name=\"w0\")\n",
    "        self.b0 = shared(0.01 * rng.randn(n_hidden), name=\"b0\")\n",
    "        self.w1 = shared(0.01 * rng.randn(n_hidden, 2 * (n_topics)).ravel(), name=\"w1\")\n",
    "        self.b1 = shared(0.01 * rng.randn(2 * (n_topics)), name=\"b1\")\n",
    "        self.rng = MRG_RandomStreams(seed=random_seed)\n",
    "        self.p_corruption = p_corruption\n",
    "\n",
    "    def encode(self, xs):\n",
    "        if 0 < self.p_corruption:\n",
    "            dixs, vixs = xs.nonzero()\n",
    "            mask = tt.set_subtensor(\n",
    "                tt.zeros_like(xs)[dixs, vixs],\n",
    "                self.rng.binomial(size=dixs.shape, n=1, p=1 - self.p_corruption),\n",
    "            )\n",
    "            xs_ = xs * mask\n",
    "        else:\n",
    "            xs_ = xs\n",
    "\n",
    "        w0 = self.w0.reshape((self.n_words, self.n_hidden))\n",
    "        w1 = self.w1.reshape((self.n_hidden, 2 * (self.n_topics)))\n",
    "        hs = tt.tanh(xs_.dot(w0) + self.b0)\n",
    "        zs = hs.dot(w1) + self.b1\n",
    "        zs_mean = zs[:, : (self.n_topics)]\n",
    "        zs_rho = zs[:, (self.n_topics) :]\n",
    "        return {\"mu\": zs_mean, \"rho\": zs_rho}\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.w0, self.b0, self.w1, self.b1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 1.255s.\n",
      "Extracting tf features for LDA...\n",
      "done in 1.427s.\n"
     ]
    }
   ],
   "source": [
    "n_words = 1000\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "data_samples = dataset.data\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_words, stop_words=\"english\")\n",
    "\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 253937 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs for training = 10000\n",
      "Number of docs for test = 1314\n",
      "Number of tokens in training set = 480280\n",
      "Sparsity = 0.0253937\n"
     ]
    }
   ],
   "source": [
    "n_samples_tr = 10000\n",
    "n_samples_te = tf.shape[0] - n_samples_tr\n",
    "docs_tr = tf[:n_samples_tr, :]\n",
    "docs_te = tf[n_samples_tr:, :]\n",
    "print(\"Number of docs for training = {}\".format(docs_tr.shape[0]))\n",
    "print(\"Number of docs for test = {}\".format(docs_te.shape[0]))\n",
    "\n",
    "n_tokens = np.sum(docs_tr[docs_tr.nonzero()])\n",
    "print(f\"Number of tokens in training set = {n_tokens}\")\n",
    "print(\n",
    "    \"Sparsity = {}\".format(len(docs_tr.nonzero()[0]) / float(docs_tr.shape[0] * docs_tr.shape[1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp_lda_doc(beta, theta, h):\n",
    "    \"\"\"Returns the log-likelihood function for given documents.\n",
    "\n",
    "    K : number of topics in the model\n",
    "    V : number of words (size of vocabulary)\n",
    "    D : number of documents (in a mini-batch)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : tensor (K x V)\n",
    "        Word distributions.\n",
    "    theta : tensor (D x K)\n",
    "        Topic distributions for documents.\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    \\log p(d | theta, beta) = \\sum_w \\log p(w | theta, beta)\n",
    "                            = \\sum_w \\log Poisson(w | theta @ beta) \n",
    "                            = \\sum_w \\log Poisson(w | \\sum_k theta_k * beta_k)\n",
    "                            = \\sum_w \\log Poisson(w | \\sum_k \\exp( \\log theta_k + \\log beta_k ))\n",
    "                            = \\sum_w - \\sum_k \\exp( \\log theta_k + \\log beta_k ) + w * \\log \\sum_k \\exp( \\log theta_k + \\log beta_k ) - \\log gamma(w + 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def ll_docs_f(docs):\n",
    "        dixs, vixs = docs.nonzero()\n",
    "        vfreqs = docs[dixs, vixs]\n",
    "\n",
    "        ll_docs = (\n",
    "            ((theta * h)[dixs] + beta.T[vixs]).sum(1) + \n",
    "            vfreqs * pmmath.logsumexp(tt.log((theta * h)[dixs]) + tt.log(beta.T[vixs]), axis=1).ravel() - \\\n",
    "            pm.distributions.special.gammaln(vfreqs + 1)\n",
    "        )\n",
    "        # Per-word log-likelihood times num of tokens in the whole dataset\n",
    "        return tt.sum(ll_docs) / (tt.sum(vfreqs) + 1e-9) * n_tokens\n",
    "\n",
    "    return ll_docs_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutianc/miniconda3/lib/python3.7/site-packages/pymc3/data.py:246: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  self.shared = theano.shared(data[in_memory_slc])\n",
      "/home/liutianc/miniconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/home/liutianc/miniconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/home/liutianc/miniconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [h]\n",
      ">Metropolis: [h0]\n",
      ">Metropolis: [w]\n",
      ">Metropolis: [theta]\n",
      ">Metropolis: [gamma]\n",
      ">Metropolis: [gamma0]\n",
      ">Metropolis: [beta]\n",
      "Sampling 4 chains, 0 divergences:   0%|          | 0/6000 [00:00<?, ?draws/s]/home/liutianc/miniconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/home/liutianc/miniconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/home/liutianc/miniconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "/home/liutianc/miniconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n",
      "Sampling 4 chains, 0 divergences: 100%|██████████| 6000/6000 [01:19<00:00, 75.08draws/s]\n",
      "/home/liutianc/miniconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n"
     ]
    }
   ],
   "source": [
    "n_topics = 20\n",
    "n_subtopics = 10\n",
    "minibatch_size = 128\n",
    "\n",
    "doc_t_minibatch = pm.Minibatch(docs_te.toarray(), minibatch_size)\n",
    "doc_t = shared(docs_te.toarray()[:minibatch_size])\n",
    "\n",
    "\n",
    "e0 = c0 = 1.\n",
    "f0 = .01\n",
    "pn = .5\n",
    "\n",
    "with pm.Model() as model:\n",
    "    beta = Dirichlet(\n",
    "        \"beta\",\n",
    "        a=pm.floatX((1.0 / n_topics) * np.ones((n_topics, n_words))),\n",
    "        shape=(n_topics, n_words),\n",
    "    )\n",
    "\n",
    "    gamma0 = Gamma(\n",
    "        \"gamma0\", \n",
    "        alpha=pm.floatX(e0 * np.ones((1, n_topics))),\n",
    "        beta=pm.floatX(f0 * np.ones((1, n_topics))),\n",
    "        shape=(minibatch_size, n_topics)\n",
    "    )\n",
    "    \n",
    "    gamma = Gamma(\n",
    "        \"gamma\",\n",
    "        alpha=gamma0, \n",
    "        beta=1 / 0.001,\n",
    "        shape=(minibatch_size, n_topics)\n",
    "    )\n",
    "\n",
    "    theta = Gamma(\n",
    "        \"theta\",\n",
    "        alpha=gamma,\n",
    "        beta=pm.floatX((pn / (1. - pn)) * np.ones((minibatch_size, n_topics))),\n",
    "        shape=(minibatch_size, n_topics),\n",
    "        total_size=n_samples_tr,\n",
    "    )\n",
    "    \n",
    "    w = Normal(\n",
    "        'w',\n",
    "        mu=0.,\n",
    "        sigma=pm.floatX(10. * np.ones((minibatch_size, n_subtopics))),\n",
    "        shape=(minibatch_size, n_subtopics)\n",
    "    )\n",
    "    \n",
    "    h0 = Bernoulli(\n",
    "        'h0',\n",
    "        p=0.5,\n",
    "        shape=(n_subtopics, n_topics),\n",
    "    )\n",
    "    \n",
    "    h = Bernoulli(\n",
    "        'h',\n",
    "        logit_p=w @ h0,\n",
    "        shape=(minibatch_size, n_topics)\n",
    "    )\n",
    "    \n",
    "    # Note, that we defined likelihood with scaling, so here we need no additional `total_size` kwarg\n",
    "    doc = pm.DensityDist(\"doc\", logp_lda_doc(beta, theta, h), observed=doc_t)\n",
    "\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(1000, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
